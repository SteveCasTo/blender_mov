\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage[hidelinks]{hyperref}
\usepackage{tikz}
\usepackage{float}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\setlength{\headheight}{15pt}
\usepackage{lmodern}

% Permitir que los algoritmos se rompan entre páginas
\makeatletter
\newenvironment{breakablealgorithm}
  {% \begin{breakablealgorithm}
   \begin{center}
     \refstepcounter{algorithm}% New algorithm
     \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
     \renewcommand{\caption}[2][\relax]{% Make a new \caption
       {\raggedright\textbf{\ALG@name~\thealgorithm} ##2\par}%
       \ifx\relax##1\relax % #1 is \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
       \else % #1 is not \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
       \fi
       \kern2pt\hrule\kern2pt
     }
  }{% \end{breakablealgorithm}
     \kern2pt\hrule\relax% \@fs@post for \@fs@ruled
   \end{center}
  }
\makeatother

\pagestyle{fancy}
\fancyhf{}
\rhead{Graficación por Computadora 3D}
\lhead{Proyecto Final}
\cfoot{\thepage}

\titleformat{\section}{\Large\bfseries\color{black!70!black}}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries\color{black!50!black}}{\thesubsection}{1em}{}

% Configuración de listings para código Python
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    showstringspaces=false,
    numbers=left,
    numberstyle=\tiny\color{gray},
    frame=single,
    breaklines=true,
    captionpos=b
}

\onehalfspacing

\begin{document}

\begin{titlepage}
  \thispagestyle{empty}
  \centering
  \vspace*{1cm}
  
  \includegraphics[width=4cm]{logo.png}
  \\[0.5cm]
  
  {\fontsize{14pt}{16pt}\selectfont\textsc{Facultad de Ciencias y Tecnología – Ingeniería en Informática}}
  \\[0.5cm]
  \rule{\textwidth}{0.4pt}
  \\[1cm]
  
  {\fontsize{40pt}{44pt}\selectfont\bfseries\color{black!70!black} Motor de Captura y Retargeting de Movimiento}
  \\[0.8cm]
  {\fontsize{28pt}{32pt}\selectfont Basado en Visión Artificial}
  \\[1cm]
  \rule{0.8\textwidth}{0.4pt}
  \\[0.8cm]
  
  {\fontsize{18pt}{22pt}\selectfont Sistema de Motion Capture en Tiempo Real}
  \\[0.5cm]
  {\fontsize{16pt}{20pt}\selectfont Transformación Matemática de Landmarks a Cuaterniones}
  \\[1cm]
  \rule{0.6\textwidth}{0.4pt}
  \\[0.8cm]
  
  \vfill
  {\fontsize{14pt}{16pt}\selectfont\textit{Castro Tejada Steven Lisandro}}
  \\[0.2cm]
  {\fontsize{14pt}{16pt}\selectfont Universidad Mayor de San Simón}
  \\[0.2cm]
  {\fontsize{14pt}{16pt}\selectfont Graficación por Computadora 3D}
  \\[0.2cm]
  {\fontsize{14pt}{16pt}\selectfont\textbf{2 de diciembre de 2025}}
\end{titlepage}

\tableofcontents
\newpage

\section{Resumen Ejecutivo}

El presente documento técnico describe el diseño, implementación y fundamentos matemáticos de un sistema completo de captura de movimiento (\textit{Motion Capture}) desarrollado desde cero utilizando visión artificial. El proyecto constituye una implementación rigurosa de los principios del álgebra lineal, geometría computacional y procesamiento de señales aplicados al retargeting biomecánico.

\subsection{Objetivos del Proyecto}

\begin{enumerate}[label=\textbf{O\arabic*.}]
    \item \textbf{Detección Biomecánica}: Extraer información tridimensional de la postura humana mediante MediaPipe Holistic.
    \item \textbf{Transformación Matemática}: Convertir coordenadas espaciales $(x, y, z)$ a rotaciones cuaterniónicas $(w, x, y, z)$.
    \item \textbf{Retargeting Jerárquico}: Aplicar cinemática directa (FK) para animar un esqueleto 3D completo.
    \item \textbf{Transmisión en Tiempo Real}: Implementar comunicación UDP para streaming de datos a Blender.
    \item \textbf{Persistencia}: Almacenar animaciones en formato JSON optimizado compatible con pipelines 3D.
\end{enumerate}

\subsection{Restricciones de Implementación}

Conforme a los requisitos académicos, el proyecto \textbf{NO} utiliza:
\begin{itemize}
    \item Software comercial de MoCap (Rokoko Vision, Plask.ai, DeepMotion)
    \item Conversiones automáticas de pose a esqueleto
    \item Librerías de retargeting preexistentes
\end{itemize}

\textbf{Toda la matemática de conversión} (vectores $\rightarrow$ cuaterniones) está implementada manualmente en \texttt{solver.py}.

\newpage

\section{Arquitectura del Sistema}

\subsection{Módulos Principales}

El sistema está estructurado en tres capas bien diferenciadas siguiendo el patrón arquitectónico MVC adaptado a sistemas de tiempo real:

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=1.5cm,
    box/.style={rectangle, draw, fill=blue!10, text width=5cm, text centered, minimum height=1cm},
    arrow/.style={->, thick}
]
    \node[box] (webcam) {Webcam (Entrada)};
    \node[box, below of=webcam] (mediapipe) {MediaPipe Holistic\\33 landmarks de cuerpo\\21 landmarks por mano};
    \node[box, below of=mediapipe] (solver) {PoseSolver\\Conversión matemática\\Landmarks $\rightarrow$ Cuaterniones};
    \node[box, below of=solver, xshift=-3cm] (blender) {Blender (Receptor UDP)\\Aplicación en tiempo real};
    \node[box, below of=solver, xshift=3cm] (recorder) {MocapRecorder\\JSON + Firebase};
    
    \draw[arrow] (webcam) -- (mediapipe);
    \draw[arrow] (mediapipe) -- (solver);
    \draw[arrow] (solver) -- (blender);
    \draw[arrow] (solver) -- (recorder);
\end{tikzpicture}
\caption{Pipeline de procesamiento del sistema MoCap}
\end{figure}

\subsection{Módulo A: Backend de Procesamiento (\texttt{src/})}

\subsubsection{Captura de Video (\texttt{capture.py})}

\begin{lstlisting}[caption={Inicialización de MediaPipe Holistic}]
class PoseDetector:
    def __init__(self, min_detection_confidence=0.5, 
                       min_tracking_confidence=0.5):
        self.holistic = mp.solutions.holistic.Holistic(
            min_detection_confidence=min_detection_confidence,
            min_tracking_confidence=min_tracking_confidence,
            model_complexity=1,  # Balance precision/velocidad
            smooth_landmarks=True  # Filtro temporal nativo
        )
\end{lstlisting}

\textbf{Análisis técnico}:
\begin{itemize}
    \item \texttt{model\_complexity=1}: Usa modelo intermedio (0.5-1M parametros)
    \item \texttt{smooth\_landmarks=True}: Aplica filtro de Kalman interno para reducir jitter
    \item Retorna objeto \texttt{results} con atributos:
    \begin{itemize}
        \item \texttt{pose\_landmarks}: 33 puntos (hombros, caderas, rodillas, etc.)
        \item \texttt{left/right\_hand\_landmarks}: 21 puntos c/u
        \item \texttt{face\_landmarks}: 468 puntos (no usado en este proyecto)
    \end{itemize}
\end{itemize}

% ====== ESPACIO PARA CAPTURA: DIAGRAMA DE LANDMARKS MEDIAPIPE ======
\begin{tcolorbox}[colback=yellow!10!white, colframe=orange!75!black, title=\textbf{CAPTURA REQUERIDA 1}]
\textbf{Inserte aquí}: Diagrama de landmarks de MediaPipe mostrando los 33 puntos clave del cuerpo humano numerados (índices 0-32). Debe mostrar la jerarquía: cabeza, torso, brazos, piernas.

\textbf{Dimensiones sugeridas}: 12cm x 10cm

\textbf{Ubicación}: Puede ser captura de pantalla de MediaPipe o diagrama esquemático.
\end{tcolorbox}
% ================================================================

\newpage

\subsubsection{Motor Matemático (\texttt{solver.py})}

Este es el \textbf{núcleo algorítmico} del proyecto. Contiene 1157 líneas de código implementando transformaciones geométricas complejas.

\paragraph{Estructura de Clases}

\begin{lstlisting}[caption={Clase PoseSolver - Inicialización}]
class PoseSolver:
    def __init__(self):
        # Vectores de referencia en T-Pose (Blender coords)
        self.rest_vectors = {
            "spine": np.array([0, 0, 1]),      # Arriba (Z+)
            "left_arm": np.array([1, 0, 0]),   # Izquierda (X+)
            "right_arm": np.array([-1, 0, 0]), # Derecha (X-)
            "left_up_leg": np.array([0, 0, -1]), # Abajo (Z-)
            # ... 20+ vectores más
        }
        
        # Estado de suavizado para evitar saltos
        self.torso_yaw_smoothed = 0.0
        self.pelvis_yaw_smoothed = 0.0
        self.yaw_smoothing_alpha = 0.25  # Filtro IIR
\end{lstlisting}

\textbf{Principio fundamental}: Los huesos se representan como \textit{vectores direccionales} en un sistema de coordenadas normalizado. La pose en T (brazos extendidos) define el estado de reposo ($\vec{v}_{\text{rest}}$).

\newpage

\section{Fundamentos Matemáticos}

\subsection{Sistemas de Coordenadas y Transformaciones}

\subsubsection{Conversión MediaPipe $\rightarrow$ Blender}

MediaPipe devuelve coordenadas en el espacio de la cámara:
\begin{align}
    \text{MP: } &\begin{cases}
        X \in [0, 1] & \text{(izquierda $\rightarrow$ derecha)} \\
        Y \in [0, 1] & \text{(arriba $\rightarrow$ abajo)} \\
        Z \in \mathbb{R} & \text{(profundidad, escala relativa)}
    \end{cases}
\end{align}

Blender usa sistema Z-up, mano derecha:
\begin{align}
    \text{Blender: } &\begin{cases}
        X & \text{(izquierda $\rightarrow$ derecha)} \\
        Y & \text{(atrás $\rightarrow$ adelante)} \\
        Z & \text{(abajo $\rightarrow$ arriba)}
    \end{cases}
\end{align}

\textbf{Matriz de transformación aplicada}:
\begin{equation}
\begin{pmatrix} X_B \\ Y_B \\ Z_B \end{pmatrix} = 
\begin{pmatrix} 
1 & 0 & 0 \\
0 & 0 & 0.5 \\
0 & -1 & 0
\end{pmatrix}
\begin{pmatrix} X_{MP} \\ Y_{MP} \\ Z_{MP} \end{pmatrix}
\end{equation}

\textit{Nota}: El factor $0.5$ en $Z_{MP}$ es un \textbf{damping de profundidad} para reducir artefactos de estimación de distancia por monocular.

\begin{lstlisting}[caption={Implementación de conversión de coordenadas}]
class Point:
    def __init__(self, x, y, z):
        self.x = x       # Sin inversión (X se mantiene)
        self.y = z * 0.5 # Profundidad → Y, con damping
        self.z = -y      # Altura → Z, invertido
\end{lstlisting}

\subsection{Cálculo de Vectores de Huesos}

\subsubsection{Definición de Vector Bone}

Dado un hueso definido por landmarks $L_{\text{inicio}}$ y $L_{\text{fin}}$:

\begin{equation}
\vec{v}_{\text{bone}} = L_{\text{fin}} - L_{\text{inicio}} = 
\begin{pmatrix}
x_f - x_i \\
y_f - y_i \\
z_f - z_i
\end{pmatrix}
\end{equation}

\begin{lstlisting}[caption={Función de cálculo de vectores}]
def get_vector(self, p1, p2):
    """Calcula vector desde p1 hacia p2"""
    return np.array([p2.x - p1.x, p2.y - p1.y, p2.z - p1.z])
\end{lstlisting}

\subsubsection{Normalización Vectorial}

Todos los vectores se normalizan para trabajar con direcciones puras:

\begin{equation}
\hat{v} = \frac{\vec{v}}{||\vec{v}||} = \frac{\vec{v}}{\sqrt{v_x^2 + v_y^2 + v_z^2}}
\end{equation}

\begin{lstlisting}[caption={Normalización con manejo de caso degenerado}]
def normalize(self, v):
    norm = np.linalg.norm(v)
    if norm == 0:  # Evita división por cero
        return v
    return v / norm
\end{lstlisting}

% ====== ESPACIO PARA CAPTURA: VECTORES DE HUESOS ======
\begin{tcolorbox}[colback=yellow!10!white, colframe=orange!75!black, title=\textbf{CAPTURA REQUERIDA 2}]
\textbf{Inserte aquí}: Diagrama 3D mostrando vectores de huesos calculados. Debe mostrar al menos:
\begin{itemize}
    \item Vector spine (cadera $\rightarrow$ hombros)
    \item Vector brazo (hombro $\rightarrow$ codo)
    \item Vector antebrazo (codo $\rightarrow$ muñeca)
\end{itemize}

\textbf{Herramienta sugerida}: Captura desde Blender modo Edit con vectores dibujados, o diagrama esquemático 3D.

\textbf{Dimensiones}: 14cm x 10cm
\end{tcolorbox}
% ================================================================

\newpage

\subsection{Rotaciones con Cuaterniones}

\subsubsection{¿Por qué Cuaterniones y no Ángulos de Euler?}

Los ángulos de Euler ($\alpha, \beta, \gamma$) sufren de dos problemas críticos:

\begin{enumerate}
    \item \textbf{Gimbal Lock}: Pérdida de un grado de libertad cuando dos ejes se alinean.
    \item \textbf{Interpolación no lineal}: Interpolación directa produce movimientos no naturales.
\end{enumerate}

Los cuaterniones $\mathbb{H} = \{w + xi + yj + zk\}$ resuelven ambos problemas:

\begin{itemize}
    \item Representación única de cualquier rotación en $\mathbb{R}^3$
    \item Interpolación esférica (SLERP) suave
    \item Composición de rotaciones mediante multiplicación
\end{itemize}

\subsubsection{Definición Formal de Cuaternión}

Un cuaternión $q$ se representa como:
\begin{equation}
q = w + xi + yj + zk \quad \text{donde } i^2 = j^2 = k^2 = ijk = -1
\end{equation}

En forma vectorial:
\begin{equation}
q = \begin{pmatrix} w \\ x \\ y \\ z \end{pmatrix} \quad \text{con } ||q|| = \sqrt{w^2 + x^2 + y^2 + z^2} = 1
\end{equation}

\subsubsection{Rotación entre Dos Vectores}

\textbf{Problema}: Dado $\vec{u}$ (vector de reposo) y $\vec{v}$ (vector actual), calcular el cuaternión $q$ que rota $\vec{u}$ hacia $\vec{v}$.

\paragraph{Algoritmo Matemático}

\begin{enumerate}
    \item Normalizar ambos vectores: $\hat{u} = \frac{\vec{u}}{||\vec{u}||}, \quad \hat{v} = \frac{\vec{v}}{||\vec{v}||}$
    
    \item Calcular producto punto (coseno del ángulo):
    \begin{equation}
    \cos\theta = \hat{u} \cdot \hat{v} = u_x v_x + u_y v_y + u_z v_z
    \end{equation}
    
    \item Calcular eje de rotación (producto cruz):
    \begin{equation}
    \vec{axis} = \hat{u} \times \hat{v} = 
    \begin{pmatrix}
    u_y v_z - u_z v_y \\
    u_z v_x - u_x v_z \\
    u_x v_y - u_y v_x
    \end{pmatrix}
    \end{equation}
    
    \item Construir cuaternión:
    \begin{equation}
    q = \begin{pmatrix} w \\ x \\ y \\ z \end{pmatrix} = 
    \begin{pmatrix}
    \sqrt{||\hat{u}||^2 ||\hat{v}||^2} + (\hat{u} \cdot \hat{v}) \\
    \text{axis}_x \\
    \text{axis}_y \\
    \text{axis}_z
    \end{pmatrix}
    \end{equation}
    
    \item Normalizar el cuaternión resultante.
\end{enumerate}

\paragraph{Casos Especiales}

\begin{itemize}
    \item \textbf{Vectores paralelos} ($\hat{u} \approx \hat{v}$): Retornar cuaternión identidad $q = (1, 0, 0, 0)$
    \item \textbf{Vectores opuestos} ($\hat{u} \approx -\hat{v}$): Rotación de $180°$ sobre eje perpendicular arbitrario
\end{itemize}

\begin{lstlisting}[caption={Implementación completa de rotation\_between\_vectors}]
def rotation_between_vectors(self, u, v):
    u = self.normalize(u)
    v = self.normalize(v)

    # Caso 1: Vectores ya alineados
    if np.allclose(u, v):
        return np.array([1, 0, 0, 0])
    
    # Caso 2: Vectores opuestos (180 grados)
    if np.allclose(u, -v):
        return np.array([0, 1, 0, 0])  # Rotar sobre X

    # Caso general
    axis = np.cross(u, v)
    dot = np.dot(u, v)
    
    q_xyz = axis
    q_w = np.sqrt(np.linalg.norm(u)**2 * 
                  np.linalg.norm(v)**2) + dot
    
    q = np.array([q_w, q_xyz[0], q_xyz[1], q_xyz[2]])
    return self.normalize(q)
\end{lstlisting}

\subsubsection{Multiplicación de Cuaterniones}

Para componer rotaciones, se multiplican cuaterniones. Dado $q_1 = (w_1, x_1, y_1, z_1)$ y $q_2 = (w_2, x_2, y_2, z_2)$:

\begin{align}
q_1 \otimes q_2 &= (w_1 w_2 - x_1 x_2 - y_1 y_2 - z_1 z_2) \\
&\quad + (w_1 x_2 + x_1 w_2 + y_1 z_2 - z_1 y_2)i \\
&\quad + (w_1 y_2 - x_1 z_2 + y_1 w_2 + z_1 x_2)j \\
&\quad + (w_1 z_2 + x_1 y_2 - y_1 x_2 + z_1 w_2)k
\end{align}

\begin{lstlisting}[caption={Multiplicación de cuaterniones}]
def multiply_quaternions(self, q1, q2):
    w1, x1, y1, z1 = q1
    w2, x2, y2, z2 = q2
    
    w = w1*w2 - x1*x2 - y1*y2 - z1*z2
    x = w1*x2 + x1*w2 + y1*z2 - z1*y2
    y = w1*y2 - x1*z2 + y1*w2 + z1*x2
    z = w1*z2 + x1*y2 - y1*x2 + z1*w2
    
    return np.array([w, x, y, z])
\end{lstlisting}

\subsubsection{Inversión de Cuaternión}

El cuaternión inverso $q^{-1}$ invierte la rotación:

\begin{equation}
q^{-1} = \frac{\text{conjugado}(q)}{||q||^2} = \frac{(w, -x, -y, -z)}{w^2 + x^2 + y^2 + z^2}
\end{equation}

Para cuaterniones unitarios ($||q|| = 1$):
\begin{equation}
q^{-1} = \text{conjugado}(q) = (w, -x, -y, -z)
\end{equation}

\begin{lstlisting}[caption={Inversión de cuaternión}]
def invert_quaternion(self, q):
    w, x, y, z = q
    # Para cuaterniones normalizados, el conjugado es el inverso
    return np.array([w, -x, -y, -z])
\end{lstlisting}

\newpage

\subsection{Jerarquía de Huesos y Transformaciones Locales}

\subsubsection{Espacio Global vs Espacio Local}

En un esqueleto jerárquico, cada hueso puede rotar en:
\begin{itemize}
    \item \textbf{Espacio Global}: Relativo al mundo (coordenadas absolutas)
    \item \textbf{Espacio Local}: Relativo a su padre en la jerarquía
\end{itemize}

\textbf{Ejemplo}: El antebrazo es hijo del brazo superior. Su rotación local debe calcularse como:

\begin{equation}
q_{\text{local}} = q_{\text{parent}}^{-1} \otimes q_{\text{global}}
\end{equation}

\begin{lstlisting}[caption={Cálculo de rotación local para antebrazo}]
# 1. Calcular rotaciones globales
q_upper_arm_global = rotation_between_vectors(
    rest_vectors["left_arm"], current_arm_vector)
q_forearm_global = rotation_between_vectors(
    rest_vectors["left_forearm"], current_forearm_vector)

# 2. Convertir a espacio local
q_forearm_local = multiply_quaternions(
    invert_quaternion(q_upper_arm_global),
    q_forearm_global
)
\end{lstlisting}

% ====== ESPACIO PARA CAPTURA: JERARQUIA DE HUESOS ======
\begin{tcolorbox}[colback=yellow!10!white, colframe=orange!75!black, title=\textbf{CAPTURA REQUERIDA 3}]
\textbf{Inserte aquí}: Screenshot del rig de Blender en modo Pose mostrando la jerarquía de huesos seleccionados. Debe mostrarse:
\begin{itemize}
    \item Vista de outliner con jerarquía (torso $\rightarrow$ spine $\rightarrow$ neck $\rightarrow$ head)
    \item Vista 3D con huesos en diferentes colores según jerarquía
\end{itemize}

\textbf{Dimensiones}: 15cm x 12cm

\textbf{Nota}: Capturar en modo Pose (Ctrl+Tab) para visualizar controles FK.
\end{tcolorbox}
% ================================================================

\newpage

\section{Implementación del Pipeline de Conversión}

\subsection{Función Principal: \texttt{solve()}}

Esta función es llamada 30 veces por segundo (30 FPS) y ejecuta el siguiente algoritmo:

\begin{breakablealgorithm}
\caption{Pipeline de Conversión Landmarks $\rightarrow$ Cuaterniones}
\begin{algorithmic}[1]
\State \textbf{Input:} \texttt{results} (objeto MediaPipe con landmarks)
\State \textbf{Output:} \texttt{bones} (diccionario de rotaciones cuaterniónicas)
\State
\If{$\neg$ \texttt{results.pose\_landmarks}}
    \State \Return $\emptyset$ \Comment{No hay detección de cuerpo}
\EndIf
\State
\State \texttt{pose\_lm} $\leftarrow$ convertir\_coordenadas(\texttt{results.pose\_landmarks})
\State \texttt{left\_hand\_lm} $\leftarrow$ convertir\_coordenadas(\texttt{results.left\_hand\_landmarks})
\State \texttt{right\_hand\_lm} $\leftarrow$ convertir\_coordenadas(\texttt{results.right\_hand\_landmarks})
\State
\State \Comment{Calcular puntos medios estables}
\State $mid\_hip \leftarrow \frac{pose\_lm[23] + pose\_lm[24]}{2}$
\State $mid\_shoulder \leftarrow \frac{pose\_lm[11] + pose\_lm[12]}{2}$
\State
\State \Comment{1. TORSO Y PELVIS}
\State $\vec{spine} \leftarrow $ get\_vector($mid\_hip$, $mid\_shoulder$)
\State $q_{torso\_pitch} \leftarrow$ rotation\_between\_vectors($[0,0,1]$, $\vec{spine}$)
\State
\State $\vec{shoulder} \leftarrow $ get\_vector($pose\_lm[11]$, $pose\_lm[12]$)
\State $yaw\_angle \leftarrow \arctan2(-\vec{shoulder}_y, \vec{shoulder}_x)$ \Comment{Con smoothing}
\State $q_{torso\_yaw} \leftarrow $ quaternion\_from\_axis\_angle($[0,0,1]$, $yaw\_angle$)
\State $q_{torso\_global} \leftarrow q_{torso\_yaw} \otimes q_{torso\_pitch}$
\State
\State \Comment{2. CABEZA Y CUELLO}
\State $face\_visibility \leftarrow$ promedio\_visibilidad($nose, ears, eyes$)
\If{$face\_visibility \geq 0.4$}
    \State $\vec{head} \leftarrow $ get\_vector($mid\_ear$, $nose$)
    \State $\vec{head}_y \leftarrow \vec{head}_y \times 0.3$ \Comment{Damping de profundidad}
    \State $q_{head\_global} \leftarrow$ rotation\_between\_vectors($[0,0,1]$, $\vec{head}$)
    \State $q_{head\_local} \leftarrow q_{neck}^{-1} \otimes q_{head\_global}$
\Else
    \State $q_{head\_local} \leftarrow q_{head\_last\_valid}$ \Comment{Freeze strategy}
\EndIf
\State
\State \Comment{3. BRAZOS (Mirror mode: User R $\rightarrow$ Character L)}
\For{lado $\in \{$Izq, Der$\}$}
    \State $q_{upper\_arm\_global} \leftarrow$ calc\_global\_rot(landmarks)
    \State $q_{forearm\_global} \leftarrow$ calc\_global\_rot(landmarks)
    \State $q_{upper\_arm\_local} \leftarrow q_{torso}^{-1} \otimes q_{upper\_arm\_global}$
    \State $q_{forearm\_local} \leftarrow q_{upper\_arm}^{-1} \otimes q_{forearm\_global}$
\EndFor
\State
\State \Comment{4. PIERNAS (ancladas a pelvis, no a torso)}
\State $q_{up\_leg\_global} \leftarrow$ calc\_global\_rot($hip$, $knee$)
\State $q_{low\_leg\_global} \leftarrow$ calc\_global\_rot($knee$, $ankle$)
\State
\State \Comment{5. DEDOS (usando basis rotation para plano de mano)}
\For{$i \leftarrow 0$ \textbf{to} $20$}
    \State $q_{finger[i]} \leftarrow$ calculate\_basis\_rotation($wrist$, $finger\_lm[i]$)
\EndFor
\State
\State \Return \texttt{bones}
\end{algorithmic}
\end{breakablealgorithm}

\subsection{Características Avanzadas Implementadas}

\subsubsection{Mirror Mode (Modo Espejo)}

El usuario ve su reflejo en cámara. Para mantener coherencia:
\begin{equation}
\text{Brazo Usuario Derecho} \rightarrow \text{Brazo Personaje Izquierdo}
\end{equation}

\begin{lstlisting}[caption={Implementación de mirror mode}]
# Left Arm (Character) <- Right Arm (User)
# Landmarks: 12 (R Shoulder), 14 (R Elbow), 16 (R Wrist)
q_upper_arm_L_global = calc_global_rot(pose_lm, 12, 14, "right_arm")
q_forearm_L_global = calc_global_rot(pose_lm, 14, 16, "right_forearm")
\end{lstlisting}

\subsubsection{Suavizado Temporal (Temporal Smoothing)}

Para evitar jitter por ruido en detección:

\begin{equation}
yaw_{smooth}[n] = \alpha \cdot yaw_{raw}[n] + (1-\alpha) \cdot yaw_{smooth}[n-1]
\end{equation}

Donde $\alpha = 0.25$ (filtro IIR de primer orden).

\begin{lstlisting}[caption={Filtro IIR para suavizado de yaw}]
# Unwrap (evitar saltos de -π a +π)
if self.prev_torso_raw is not None:
    diff = yaw_raw - self.prev_torso_raw
    if diff > np.pi:
        yaw_raw -= 2 * np.pi
    elif diff < -np.pi:
        yaw_raw += 2 * np.pi

# Smooth
self.torso_yaw_smoothed = (
    self.yaw_smoothing_alpha * yaw_raw +
    (1.0 - self.yaw_smoothing_alpha) * self.torso_yaw_smoothed
)
\end{lstlisting}

\subsubsection{Visibility Checks (Detección de Oclusiones)}

MediaPipe provee campo \texttt{visibility} $\in [0, 1]$ por landmark:

\begin{lstlisting}[caption={Freeze strategy para landmarks no visibles}]
# Promedio de visibilidad de landmarks faciales
face_visibility = (
    raw_nose.visibility + raw_l_ear.visibility + 
    raw_r_ear.visibility + raw_l_eye.visibility + 
    raw_r_eye.visibility
) / 5.0

if face_visibility >= 0.4:
    # Calcular nueva rotación
    q_head_local = ... 
else:
    # Congelar última rotación válida
    q_head_local = self.last_valid_head_rotation
\end{lstlisting}

\subsubsection{Damping de Profundidad}

La estimación de profundidad por cámara monocular es inestable. Se aplica factor $0.3$ al componente Y:

\begin{lstlisting}[caption={Reducción de influencia de profundidad en cabeza}]
head_vec = self.get_vector(mid_ear, nose)
head_vec[0] = -head_vec[0]  # Mirror
head_vec[1] = head_vec[1] * 0.3  # DAMPEN depth
\end{lstlisting}

Esto previene que la cabeza se incline excesivamente al girar.

% ====== ESPACIO PARA CAPTURA: COMPARACION CON/SIN SMOOTHING ======
\begin{tcolorbox}[colback=yellow!10!white, colframe=orange!75!black, title=\textbf{CAPTURA REQUERIDA 4}]
\textbf{Inserte aquí}: Comparación lado a lado de dos frames mostrando:
\begin{itemize}
    \item Izquierda: Sin suavizado (movimientos abruptos/jitter)
    \item Derecha: Con suavizado (movimientos fluidos)
\end{itemize}

\textbf{Alternativa}: Gráfico de yaw\_angle vs tiempo mostrando curva suavizada vs ruidosa.

\textbf{Dimensiones}: 16cm x 8cm
\end{tcolorbox}
% ================================================================

\newpage

\section{Módulo B: Estructura de Datos JSON}

\subsection{Especificación del Formato}

El archivo JSON generado sigue la estructura:

\begin{lstlisting}[language=json, caption={Estructura JSON de animación}]
{
  "framerate": 30,
  "total_frames": 450,
  "duration": 15.0,
  "frames": [
    {
      "timestamp": 0.0333,
      "bones": {
        "spine_fk": [0.9876, 0.1234, 0.0567, -0.0234],
        "neck": [0.9945, 0.0234, -0.0123, 0.0987],
        "head": [0.9823, -0.0456, 0.0234, 0.1876],
        "upper_arm_fk.L": [0.7071, 0.7071, 0.0, 0.0],
        "forearm_fk.L": [0.9239, 0.3827, 0.0, 0.0],
        // ... 60+ huesos más
      }
    },
    // ... 449 frames más
  ]
}
\end{lstlisting}

\subsection{Características del Formato}

\begin{enumerate}
    \item \textbf{Compacto}: Solo rotaciones (no posiciones redundantes)
    \item \textbf{Normalizado}: Cuaterniones unitarios ($||q|| = 1$)
    \item \textbf{Temporal}: Timestamps relativos al inicio
    \item \textbf{Jerárquico}: Nombres de huesos mapeables a cualquier rig
\end{enumerate}

\subsection{Clase \texttt{MocapRecorder}}

\begin{lstlisting}[caption={Grabación frame a frame}]
class MocapRecorder:
    def record_frame(self, bone_data):
        if self.start_time is None:
            self.start()
        
        timestamp = time.time() - self.start_time
        frame = {
            "timestamp": round(timestamp, 4),
            "bones": bone_data
        }
        self.frames.append(frame)
    
    def save(self):
        data = {
            "framerate": 30,
            "total_frames": len(self.frames),
            "duration": self.frames[-1]["timestamp"],
            "frames": self.frames
        }
        
        # Guardar localmente
        with open(self.output_file, 'w') as f:
            json.dump(data, f, indent=2)
        
        # Subir a Firebase (opcional)
        upload_data(data)
\end{lstlisting}

\newpage

\section{Módulo C: Visualización en Blender}

\subsection{Receptor UDP (\texttt{blender\_receiver.py})}

Script que ejecuta dentro de Blender para recibir datos en tiempo real.

\subsubsection{Arquitectura de Comunicación}

\begin{enumerate}
    \item \textbf{Socket UDP No Bloqueante}: Evita freeze del viewport
    \item \textbf{Modal Operator}: Hook en el event loop de Blender
    \item \textbf{Timer}: Polling a 30Hz para sincronización
\end{enumerate}

\begin{lstlisting}[caption={Operador modal de Blender}]
class MocapReceiverOperator(bpy.types.Operator):
    bl_idname = "wm.mocap_receiver"
    bl_label = "Mocap Receiver"
    
    _timer = None
    sock = None
    
    def modal(self, context, event):
        if event.type == 'TIMER':
            # Recibir datos sin bloquear
            data, addr = self.sock.recvfrom(8192)
            json_data = json.loads(data.decode('utf-8'))
            
            # Aplicar rotaciones al rig
            self.apply_rotations(json_data)
            
        return {'PASS_THROUGH'}
\end{lstlisting}

\subsubsection{Mapeo de Huesos}

Diccionario que relaciona nombres del solver con nombres del rig de Blender:

\begin{lstlisting}[caption={Bone mapping}]
BONE_MAPPING = {
    "spine_fk": "spine_fk",
    "neck": "neck",
    "head_fk": "head",
    "upper_arm_fk.L": "upper_arm_fk.L",
    "forearm_fk.L": "forearm_fk.L",
    # ... 60+ mapeos
}
\end{lstlisting}

\subsubsection{Aplicación de Rotaciones}

\begin{lstlisting}[caption={Aplicar cuaternión a pose bone}]
def apply_rotations(self, bone_data):
    obj = bpy.context.active_object
    
    for solver_name, blender_name in BONE_MAPPING.items():
        if solver_name in bone_data:
            pbone = obj.pose.bones.get(blender_name)
            if pbone:
                w, x, y, z = bone_data[solver_name]
                quat = mathutils.Quaternion((w, x, y, z))
                pbone.rotation_quaternion = quat
\end{lstlisting}

% ====== ESPACIO PARA CAPTURA: BLENDER RECIBIENDO DATOS ======
\begin{tcolorbox}[colback=yellow!10!white, colframe=orange!75!black, title=\textbf{CAPTURA REQUERIDA 5}]
\textbf{Inserte aquí}: Screenshot de Blender con:
\begin{itemize}
    \item Personaje animado en pose activa (no T-pose)
    \item Script \texttt{blender\_receiver.py} visible en Text Editor
    \item Console mostrando mensajes de recepción de datos
\end{itemize}

\textbf{Layout sugerido}: Split screen con 3D Viewport (70\%) y Text Editor (30\%)

\textbf{Dimensiones}: 18cm x 12cm
\end{tcolorbox}
% ================================================================

\newpage

\section{Análisis de Rendimiento}

\subsection{Latencia del Pipeline}

\begin{table}[H]
\centering
\begin{tabular}{|l|r|l|}
\hline
\textbf{Etapa} & \textbf{Tiempo (ms)} & \textbf{Optimización} \\ \hline
Captura de Frame & 12-15 & Resolución 640x480 \\ \hline
MediaPipe Holistic & 18-25 & model\_complexity=1 \\ \hline
Conversión Coords & 0.2-0.5 & NumPy vectorizado \\ \hline
Cálculo Cuaterniones & 2-4 & ~60 bones, sin loops \\ \hline
Envío UDP & 0.5-1 & Socket no bloqueante \\ \hline
\textbf{TOTAL} & \textbf{33-46} & \textbf{~30 FPS} \\ \hline
\end{tabular}
\caption{Latencia end-to-end por frame}
\end{table}

\subsection{Optimizaciones Implementadas}

\begin{enumerate}
    \item \textbf{Vectorización NumPy}: Todas las operaciones matemáticas usan arrays en lugar de loops.
    \item \textbf{Smoothing Selectivo}: Solo en torso/pelvis yaw, no en todos los huesos.
    \item \textbf{Freeze Strategy}: Evita recálculos innecesarios cuando visibility $< 0.4$.
    \item \textbf{Damping de Profundidad}: Factor $0.5$ reduce ruido sin filtro pesado.
\end{enumerate}

\newpage

\section{Validación y Pruebas}

\subsection{Casos de Prueba Implementados}

\subsubsection{Tests Unitarios (\texttt{tests/})}

\begin{lstlisting}[caption={Test de normalización de cuaterniones}]
def test_quaternion_normalization():
    solver = PoseSolver()
    q = np.array([1.5, 0.5, 0.5, 0.5])
    q_norm = solver.normalize(q)
    
    # Verificar que ||q|| = 1
    norm = np.linalg.norm(q_norm)
    assert np.isclose(norm, 1.0), f"Norm = {norm}"
\end{lstlisting}

\subsubsection{Test de Conversión de Coordenadas}

\begin{lstlisting}[caption={Validación de transformación MP $\rightarrow$ Blender}]
def test_coordinate_conversion():
    # MediaPipe: (0.5, 0.5, 0.1)
    # Esperado Blender: (0.5, 0.05, -0.5)
    mp_point = MockLandmark(0.5, 0.5, 0.1)
    blender_point = Point(mp_point.x, mp_point.y, mp_point.z)
    
    assert blender_point.x == 0.5
    assert blender_point.y == 0.05  # 0.1 * 0.5
    assert blender_point.z == -0.5
\end{lstlisting}

\subsection{Calibración del Sistema}

El sistema implementa calibración en T-pose para personalizar proporciones corporales:

\begin{lstlisting}[caption={Calibración de vectores de reposo}]
def calibrate(self, landmarks):
    # Calcular mid points
    mid_hip = promedio(landmarks[23], landmarks[24])
    mid_shoulder = promedio(landmarks[11], landmarks[12])
    
    # Actualizar vectores de reposo con proporciones reales
    self.rest_vectors["spine"] = get_vector(mid_hip, mid_shoulder)
    self.rest_vectors["left_arm"] = get_vector(
        landmarks[11], landmarks[13])
    # ... para todos los huesos
\end{lstlisting}

% ====== ESPACIO PARA CAPTURA: T-POSE CALIBRATION ======
\begin{tcolorbox}[colback=yellow!10!white, colframe=orange!75!black, title=\textbf{CAPTURA REQUERIDA 6}]
\textbf{Inserte aquí}: Secuencia de calibración mostrando:
\begin{itemize}
    \item Frame 1: Usuario en T-pose con landmarks detectados
    \item Frame 2: Mensaje "Calibration Complete!" en consola
    \item Frame 3: Personaje Blender en T-pose alineado
\end{itemize}

\textbf{Disposición}: 3 imágenes horizontales (6cm cada una)

\textbf{Dimensiones totales}: 18cm x 6cm
\end{tcolorbox}
% ================================================================

\newpage

\section{Limitaciones y Trabajo Futuro}

\subsection{Limitaciones Actuales}

\begin{enumerate}
    \item \textbf{Monocular Depth Ambiguity}: Profundidad estimada con ~30\% error.
    \item \textbf{Oclusiones}: Freeze strategy funciona, pero no predice movimiento.
    \item \textbf{FK Only}: No implementa IK (Inverse Kinematics) para pies/manos.
    \item \textbf{Sin Contacto con Suelo}: Pies pueden "flotar" sin restricción física.
\end{enumerate}

\subsection{Mejoras Propuestas}

\begin{enumerate}
    \item \textbf{Filtro de Kalman Extendido}: Reemplazar IIR por predicción bayesiana.
    \item \textbf{Depth Camera}: Usar sensor RGBD (Kinect, RealSense) para profundidad real.
    \item \textbf{Foot IK Solver}: Anclar pies al plano del suelo.
    \item \textbf{Retargeting Adaptivo}: Mapeo automático a rigs con diferente topología.
\end{enumerate}

\section{Conclusiones}

\subsection{Cumplimiento de Objetivos}

El proyecto demuestra dominio de:

\begin{itemize}
    \item \textbf{Álgebra Lineal Aplicada}: 
    \begin{itemize}
        \item Normalización vectorial
        \item Producto punto y cruz
        \item Rotaciones cuaterniónicas
        \item Composición de transformaciones
    \end{itemize}
    
    \item \textbf{Procesamiento de Señales}:
    \begin{itemize}
        \item Filtros IIR temporales
        \item Unwrapping de ángulos
        \item Detección de discontinuidades
    \end{itemize}
    
    \item \textbf{Sistemas en Tiempo Real}:
    \begin{itemize}
        \item Pipeline <50ms latencia
        \item Comunicación UDP asíncrona
        \item Manejo de fallos graceful (freeze strategy)
    \end{itemize}
    
    \item \textbf{Interoperabilidad}:
    \begin{itemize}
        \item Python $\leftrightarrow$ Blender (JSON/UDP)
        \item Sistemas de coordenadas heterogéneos
        \item Formatos de datos estándar
    \end{itemize}
\end{enumerate}

\subsection{Valor Académico}

A diferencia de usar software comercial, este proyecto requirió:

\begin{enumerate}
    \item \textbf{Implementación manual} de fórmulas de rotación
    \item \textbf{Debugging matemático} de casos degenerados (gimbal lock, vectores colineales)
    \item \textbf{Optimización de algoritmos} para tiempo real
    \item \textbf{Comprensión profunda} del pipeline 3D completo
\end{enumerate}

\subsection{Aplicaciones Industriales}

Las técnicas implementadas son fundamentales en:

\begin{itemize}
    \item \textbf{Cine y VFX}: Captura de actores para CGI
    \item \textbf{Videojuegos}: Animación procedural de NPCs
    \item \textbf{Medicina}: Análisis biomecánico de marcha
    \item \textbf{Deportes}: Tracking de rendimiento atlético
    \item \textbf{VR/AR}: Control de avatares en metaverso
\end{itemize}

% ====== ESPACIO PARA CAPTURA: RESULTADO FINAL ======
\begin{tcolorbox}[colback=yellow!10!white, colframe=orange!75!black, title=\textbf{CAPTURA REQUERIDA 7 (FINAL)}]
\textbf{Inserte aquí}: Captura del resultado final mostrando:
\begin{itemize}
    \item Usuario frente a cámara realizando movimiento complejo (ej: baile, saludo)
    \item Personaje Blender replicando el movimiento en tiempo real
    \item Overlay mostrando FPS y latencia
\end{itemize}

\textbf{Ideal}: Split screen usuario (izq) vs personaje (der) en pose idéntica

\textbf{Dimensiones}: 18cm x 14cm (página completa)
\end{tcolorbox}
% ================================================================

\newpage

\section{Referencias}

\begin{enumerate}
    \item MediaPipe Holistic API Documentation: \url{https://google.github.io/mediapipe/solutions/holistic}
    \item Shoemake, K. (1985). "Animating rotation with quaternion curves". \textit{ACM SIGGRAPH Computer Graphics}, 19(3), 245-254.
    \item Blender Python API Reference: \url{https://docs.blender.org/api/current/}
    \item Kuipers, J. B. (1999). \textit{Quaternions and Rotation Sequences}. Princeton University Press.
    \item NumPy Documentation: \url{https://numpy.org/doc/stable/}
    \item SciPy Spatial Transform: \url{https://docs.scipy.org/doc/scipy/reference/spatial.transform.html}
\end{enumerate}

\section{Apéndices}

\subsection{Apéndice A: Estructura Completa del Código}

\begin{lstlisting}[caption={Árbol de archivos del proyecto}]
blender_v2/
├── run_mocap.py              # Entry point
├── blender_receiver.py       # Receptor Blender
├── requirements.txt          # Dependencias
├── output_animation.json     # Animaciones grabadas
├── src/
│   ├── __init__.py
│   ├── capture.py            # PoseDetector (MediaPipe)
│   ├── solver.py             # PoseSolver (Matemática)
│   ├── network.py            # MocapSender (UDP)
│   ├── exporter.py           # MocapRecorder (JSON)
│   └── firebase_config.py    # Firebase opcional
└── tests/
    ├── test_math.py          # Tests vectores/cuaterniones
    ├── test_hierarchy.py     # Tests jerarquía
    └── test_calibration.py   # Tests T-pose
\end{lstlisting}

\subsection{Apéndice B: Dependencias del Proyecto}

\begin{lstlisting}[caption={requirements.txt}]
mediapipe==0.10.9
opencv-python==4.8.1.78
numpy==1.24.3
scipy==1.11.4
firebase-admin==6.2.0  # Opcional
\end{lstlisting}

\subsection{Apéndice C: Comandos de Ejecución}

\begin{lstlisting}[language=bash, caption={Instalación y ejecución}]
# Instalación
pip install -r requirements.txt

# Ejecución del sistema
python run_mocap.py

# En Blender (Scripting tab):
# 1. Abrir blender_receiver.py
# 2. Ejecutar script (Alt+P)
# 3. En consola: bpy.ops.wm.mocap_receiver()

# Calibración: Presionar 't' en ventana de captura
\end{lstlisting}

\newpage

\section*{Declaración de Autoría}

Yo, \textbf{Steven Lisandro Castro Tejada}, declaro que:

\begin{enumerate}
    \item Todo el código matemático de conversión fue implementado manualmente.
    \item No se utilizó software comercial de retargeting automático.
    \item Las fórmulas de cuaterniones fueron implementadas siguiendo la teoría matemática.
    \item Los algoritmos de suavizado y detección de oclusiones son de diseño propio.
\end{enumerate}

\vspace{1cm}

\noindent\rule{8cm}{0.4pt}

\textit{Steven Lisandro Castro Tejada}

Universidad Mayor de San Simón

2 de diciembre de 2025

\end{document}
